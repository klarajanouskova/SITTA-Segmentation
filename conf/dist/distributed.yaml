#  help='number of distributed processes'
world_size: 1
local-rank: -1
#  help='url used to set up distributed training'
dist_url: "env://"
#  help='Batch size per GPU (effective batch size is batch_size * accum_iter * # gpus'